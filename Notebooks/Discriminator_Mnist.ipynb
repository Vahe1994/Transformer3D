{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../external/Transformer_modules/')\n",
    "sys.path.append('../src/')\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from modules import MultiHeadAttention, PositionwiseFeedForward\n",
    "import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class GlobalAveragePooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.mean(dim=self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=100,ffn_dim =200,n_head=8):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.constant_(self.fc1.bias, 0.0)\n",
    "        \n",
    "       \n",
    "        \n",
    "        self.mha_1 = MultiHeadAttention(n_head=n_head,d_model = hidden_dim)\n",
    "        self.ffn_1 = PositionwiseFeedForward(hidden_dim, ffn_dim, use_residual=False)\n",
    "        \n",
    "        self.gl_1 =  GlobalAveragePooling(dim = 1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, 10)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.constant_(self.fc2.bias, 0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = self.mha_1(h1)\n",
    "        h3 = self.ffn_1(h2)\n",
    "        score = self.fc2(self.gl_1(h3))\n",
    "        return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = mnist.make_clouds(mnist.x_train,500) \n",
    "y_train = mnist.y_train\n",
    "x_val = mnist.make_clouds(mnist.x_val,500) \n",
    "y_val = mnist.y_val\n",
    "model = Discriminator(2).cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = mnist.mnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X_batch, y_batch):\n",
    "    X_batch = Variable(torch.FloatTensor(X_batch)).cuda(0)\n",
    "    y_batch = Variable(torch.LongTensor(y_batch)).cuda(0)\n",
    "    logits = model(X_batch)\n",
    "    return F.cross_entropy(logits, y_batch).mean()\n",
    "\n",
    "def iterate_minibatches(X, y, batchsize):\n",
    "    indices = np.random.permutation(np.arange(len(X)))\n",
    "    for start in range(0, len(indices), batchsize):\n",
    "        ix = indices[start: start + batchsize]\n",
    "        yield X[ix], y[ix]\n",
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_epochs = 150 # total amount of full passes over training data\n",
    "batch_size = 200\n",
    "train_loss = []\n",
    "val_accuracy = []\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train(True) \n",
    "    for X_batch, y_batch in iterate_minibatches(x_train,y_train,batchsize=batch_size):\n",
    "        # train on batch\n",
    "        loss = compute_loss(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss.append(loss.cpu().detach().numpy())\n",
    "        del loss\n",
    "\n",
    "       \n",
    "        \n",
    "    # And a full pass over the validation data:\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in iterate_minibatches(x_val, y_val, batch_size):\n",
    "        logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "        y_pred = logits.max(1)[1].cpu().detach().numpy()\n",
    "        val_accuracy.append(np.mean(y_batch == y_pred))\n",
    "        del logits\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-len(x_train) // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(val_accuracy[-len(x_val) // batch_size :]) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
