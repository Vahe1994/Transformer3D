{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../external/Transformer_modules/')\n",
    "sys.path.append('../external/Data_Pointnet++/')\n",
    "sys.path.append('../src/')\n",
    "import modelnet_dataset\n",
    "import modelnet_h5_dataset\n",
    "import os\n",
    "from files import BASE_DIR\n",
    "\n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'utils'))\n",
    "\n",
    "NUM_CLASSES = 40\n",
    "normal = False \n",
    "NUM_POINT= 1024\n",
    "BATCH_SIZE = 32\n",
    "if normal:\n",
    "    assert(NUM_POINT<=10000)\n",
    "    DATA_PATH = os.path.join(ROOT_DIR, 'data/modelnet40_normal_resampled')\n",
    "    TRAIN_DATASET = modelnet_dataset.ModelNetDataset(root=DATA_PATH, npoints=NUM_POINT, split='train', normal_channel=FLAGS.normal, batch_size=BATCH_SIZE)\n",
    "    TEST_DATASET = modelnet_dataset.ModelNetDataset(root=DATA_PATH, npoints=NUM_POINT, split='test', normal_channel=FLAGS.normal, batch_size=BATCH_SIZE)\n",
    "else:\n",
    "    assert(NUM_POINT<=2048)\n",
    "    TRAIN_DATASET = modelnet_h5_dataset.ModelNetH5Dataset(os.path.join(BASE_DIR, 'data/modelnet40_ply_hdf5_2048/train_files.txt'), batch_size=BATCH_SIZE, npoints=NUM_POINT, shuffle=True)\n",
    "    TEST_DATASET = modelnet_h5_dataset.ModelNetH5Dataset(os.path.join(BASE_DIR, 'data/modelnet40_ply_hdf5_2048/test_files.txt'), batch_size=BATCH_SIZE, npoints=NUM_POINT, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from modules import MultiHeadAttention, PositionwiseFeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "# cudnn.enabled = False\n",
    "# cudnn.benchmark = True\n",
    "\n",
    "class GlobalAveragePooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.mean(dim=self.dim)\n",
    "    \n",
    "class GlobalPooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        avg = x.mean(dim=self.dim)\n",
    "        max = x.max(dim=self.dim)[0]\n",
    "        min = x.min(dim=self.dim)[0]\n",
    "        \n",
    "        return torch.cat([min, avg, max], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim, \n",
    "                 hidden_dim=100,\n",
    "                 ffn_dim =200,\n",
    "                 n_head=8,\n",
    "                 normalize_loc=True,\n",
    "                 normalize_scale=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.normalize_loc = normalize_loc\n",
    "        self.normalize_scale = normalize_scale\n",
    "        self.dropout1  = nn.Dropout(p=0.2)\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.constant_(self.fc1.bias, 0.0)\n",
    "        self.dropout2  = nn.Dropout(p=0.1)\n",
    "        self.dropout3  = nn.Dropout(p=0.1)\n",
    "        self.dropout4  = nn.Dropout(p=0.1)\n",
    "        self.mha_1 = MultiHeadAttention(n_head=n_head,d_model = hidden_dim)\n",
    "        self.ffn_1 = PositionwiseFeedForward(hidden_dim, ffn_dim, use_residual=False)\n",
    "        self.mha_2 = MultiHeadAttention(n_head=n_head,d_model = hidden_dim)\n",
    "        self.ffn_2 = PositionwiseFeedForward(hidden_dim, ffn_dim, use_residual=False)\n",
    "        \n",
    "        self.gl_1 =  GlobalPooling(dim = 1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim * 3, 40)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.constant_(self.fc2.bias, 0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.normalize_loc:\n",
    "            x = x - x.mean(dim=1, keepdim=True)\n",
    "        if self.normalize_scale:\n",
    "            x = x / x.std(dim=1, keepdim=True)\n",
    "        \n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h1 =self.dropout1(h1) \n",
    "        h2 = self.mha_1(h1)\n",
    "        h2 = self.dropout2(h2)\n",
    "        h3 = self.ffn_1(h2)\n",
    "        h3 = self.dropout3(h3)\n",
    "        h4 = self.mha_2(h3)\n",
    "        h4 = self.dropout4(h4)\n",
    "        h5 = self.ffn_2(h4)\n",
    "        score = self.fc2(self.gl_1(h5))\n",
    "        return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Discriminator(3).cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X_batch, y_batch):\n",
    "    X_batch = Variable(torch.FloatTensor(X_batch)).cuda(0)\n",
    "    y_batch = Variable(torch.LongTensor(y_batch)).cuda(0)\n",
    "    logits = model(X_batch)\n",
    "    return F.cross_entropy(logits, y_batch).mean()\n",
    "\n",
    "def iterate_minibatches(X, y, batchsize):\n",
    "    indices = np.random.permutation(np.arange(len(X)))\n",
    "    for start in range(0, len(indices), batchsize):\n",
    "        ix = indices[start: start + batchsize]\n",
    "        yield X[ix], y[ix]\n",
    "opt = torch.optim.Adam(model.parameters(),lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from  tqdm import tqdm\n",
    "num_epochs = 150 # total amount of full passes over training data\n",
    "batch_size = 32\n",
    "train_loss = []\n",
    "val_accuracy = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    start_time = time.time()\n",
    "    model.train(True) \n",
    "    TRAIN_DATASET.reset()\n",
    "    while TRAIN_DATASET.has_next_batch():\n",
    "        batch_data, batch_label = TRAIN_DATASET.next_batch(augment=True)\n",
    "        loss = compute_loss(batch_data, batch_label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss.append(loss.cpu().detach().numpy())\n",
    "        del loss\n",
    "    \n",
    "       \n",
    "        \n",
    "    # And a full pass over the validation data:\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    TEST_DATASET.reset()\n",
    "    while TEST_DATASET.has_next_batch():\n",
    "        batch_data, batch_label = TEST_DATASET.next_batch(augment=False)\n",
    "        logits = model(Variable(torch.FloatTensor(batch_data)).cuda(0))\n",
    "        y_pred = logits.max(1)[1].cpu().detach().numpy()\n",
    "        val_accuracy.append(np.mean(batch_label == y_pred))\n",
    "        del logits\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-9840 // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(val_accuracy[-2468 // batch_size :]) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
